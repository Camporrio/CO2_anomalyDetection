{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de anomalías en la calidad del aire en dispositivos IoT en el Edge computing: un enfoque basado en datos de CO2\n",
    "\n",
    "En este proyecto, utilizaremos TensorFlow para desarrollar un modelo de detección de anomalías para la calidad del aire. Nuestro enfoque consiste en la combinación de un Autoencoder y una Red Neuronal de Larga Memoria a Corto Plazo (LSTM) para identificar anomalías en los datos de CO2 recogidos por dispositivos IoT en el Edge computing.\n",
    "\n",
    "Los datos que utilizaremos para entrenar y evaluar nuestro modelo provienen del CEIP Albea Valld'Alba, y están disponibles para su descarga en [Zenodo](https://doi.org/10.5281/zenodo.5036228). Este dataset contiene mediciones de la calidad del aire, incluyendo los niveles de CO2, y será la base para nuestro análisis y modelado.\n",
    "\n",
    "A lo largo de este notebook, procesaremos y exploraremos estos datos, entrenaremos nuestro modelo de detección de anomalías, y evaluaremos su rendimiento. El objetivo final es desarrollar un modelo robusto y efectivo que pueda identificar anomalías en la calidad del aire en tiempo real, permitiendo una respuesta rápida a cualquier cambio en las condiciones del aire.\n",
    "\n",
    "Guillem Campo Fons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA ANALYSIS AND OPERATIONS\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "#TENSORFLOW\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, Dropout, RepeatVector\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "#VISUALIZATIONS\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from numpy.random import seed\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "TRATAMIENTO = 'ELIMINACION' #ELIMINACION o LIMITACION\n",
    "NORMAL = 'ROBUST' #STANDARD o MINMAX o ROBUST\n",
    "LOSS_FUNCTION = 'mae' #mse o mae\n",
    "LOSS1 = 'MSE' #MAE o MSE\n",
    "LOSS2 = 'MSE' #MAE o MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CEIP_Albea_ValldAlba.csv', usecols=['date_time', 'co2', 'sensor_id'])\n",
    "df['date_time'] = pd.to_datetime(df['date_time']).dt.strftime('%m-%d-%Y %H:%M')\n",
    "df_sorted = df.sort_values(by='date_time')\n",
    "sensor_ids = [' CO2_02']#, ' CO2_01', ' CO2_03', ' CO2_04', ' CO2_05', ' CO2_06']\n",
    "df_filtered = []\n",
    "for sensor_id in sensor_ids:\n",
    "    df_filtered.append(df_sorted[df_sorted['sensor_id'] == sensor_id])\n",
    "for i in range(len(df_filtered)):\n",
    "    df = df_filtered[i].copy()\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df = df.drop_duplicates(subset='date_time', keep='first')\n",
    "    df = df.drop(['sensor_id'], axis=1)\n",
    "    df = df.set_index('date_time')\n",
    "    df = df.resample('5T').asfreq()\n",
    "    df = df.reset_index()\n",
    "    df_filtered[i] = df\n",
    "df_concatenated = pd.concat(df_filtered)\n",
    "df_concatenated = (df_concatenated\n",
    "                   .interpolate()\n",
    "                   .drop('date_time', axis=1)\n",
    "                   .reset_index(drop=True))\n",
    "df_concatenated_nocap = df_concatenated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "sns.lineplot(data=df_concatenated, x=df_concatenated.index, y='co2', ax=ax, linewidth=2)\n",
    "ax.set_xlabel('Order in Time', fontsize=12)\n",
    "ax.set_ylabel('CO2 (PPM)', fontsize=12)\n",
    "ax.set_title('CO2 Recordings Over Time by Sensor', fontsize=14)\n",
    "ax.set_facecolor('#F5F5F5')\n",
    "ax.grid(visible=True, linestyle='solid', alpha=0.5)\n",
    "ax.margins(x=0.02, y=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1993)\n",
    "stat, p = shapiro(df_concatenated)\n",
    "print('Estadisticos=%.3f, p=%.3f' % (stat, p))\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "   print('La muestra parece Gaussiana o Normal (no se rechaza la hipótesis nula H0)')\n",
    "else:\n",
    "   print('La muestra no parece Gaussiana o Normal(se rechaza la hipótesis nula H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = 968\n",
    "\n",
    "if(TRATAMIENTO  == 'ELIMINACION'):\n",
    " df_concatenated = df_concatenated[df_concatenated['co2'] <= upper_limit]\n",
    "if(TRATAMIENTO  == 'LIMITACION'):\n",
    " df_concatenated['co2'] = np.where(df_concatenated['co2'] > upper_limit, upper_limit, df_concatenated['co2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "sns.lineplot(data=df_concatenated, x=df_concatenated.index, y='co2', ax=ax, linewidth=2)\n",
    "ax.set_xlabel('Time Order', fontsize=12)\n",
    "ax.set_ylabel('CO2 (PPM)', fontsize=12)\n",
    "ax.set_title(f'CO2 Recordings Over Time ({TRATAMIENTO} PPM)', fontsize=14)\n",
    "ax.grid(visible=True, linestyle='solid', alpha=0.5)\n",
    "ax.margins(x=0.02, y=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(NORMAL == 'STANDARD'):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    train_scaled = scaler.fit_transform(df_concatenated)\n",
    "    test_scaled = scaler.transform(df_concatenated_nocap)\n",
    "if(NORMAL == 'MINMAX'):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    train_scaled = scaler.fit_transform(df_concatenated)\n",
    "    test_scaled = scaler.transform(df_concatenated_nocap)\n",
    "if(NORMAL == 'ROBUST'):\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    train_scaled = scaler.fit_transform(df_concatenated)\n",
    "    test_scaled = scaler.transform(df_concatenated_nocap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 3\n",
    "\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "train_sequenced = create_sequences(train_scaled)\n",
    "print(\"Training input shape: \", train_sequenced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(LSTM(16, activation='tanh', input_shape=(train_sequenced.shape[1], train_sequenced.shape[2]),return_sequences=False))\n",
    "model_1.add(RepeatVector(train_sequenced.shape[1]))\n",
    "model_1.add(LSTM(16, activation='tanh', return_sequences=True))\n",
    "model_1.add(TimeDistributed(Dense(train_sequenced.shape[2])))\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(LSTM(16, activation='tanh', input_shape=(train_sequenced.shape[1], train_sequenced.shape[2]), return_sequences=True))\n",
    "model_2.add(Dropout(rate=0.2))\n",
    "model_2.add(TimeDistributed(Dense(train_sequenced.shape[2])))\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(LSTM(64, activation='tanh', input_shape=(train_sequenced.shape[1], train_sequenced.shape[2]), return_sequences=True))\n",
    "model_3.add(Dropout(rate=0.2))\n",
    "model_3.add(LSTM(16, activation='tanh', return_sequences=True))\n",
    "model_3.add(TimeDistributed(Dense(train_sequenced.shape[2])))\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(LSTM(128, activation='tanh', input_shape=(train_sequenced.shape[1], train_sequenced.shape[2]), return_sequences=True))\n",
    "model_4.add(Dropout(rate=0.2))\n",
    "model_4.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "model_4.add(Dropout(rate=0.2))\n",
    "model_4.add(LSTM(16, activation='tanh', return_sequences=True))\n",
    "model_4.add(TimeDistributed(Dense(train_sequenced.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=LOSS_FUNCTION)\n",
    "model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=LOSS_FUNCTION)\n",
    "model_3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=LOSS_FUNCTION)\n",
    "model_4.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=LOSS_FUNCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model_1.fit(train_sequenced, train_sequenced, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT, callbacks=[early_stopping_callback]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(train_sequenced, train_sequenced, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT, callbacks=[early_stopping_callback]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3 = model_3.fit(train_sequenced, train_sequenced, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT, callbacks=[early_stopping_callback]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_4 = model_4.fit(train_sequenced, train_sequenced, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT, callbacks=[early_stopping_callback]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "histories = [history_1, history_2, history_3, history_4]\n",
    "titles = ['Model 1', 'Model 2', 'Model 3', 'Model 4']\n",
    "\n",
    "num_plots = len(histories)\n",
    "num_plots_per_row = 1\n",
    "num_rows = (num_plots + num_plots_per_row - 1) // num_plots_per_row\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_plots_per_row, figsize=(12,12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (history, title) in enumerate(zip(histories, titles)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(history['loss'], label='Training loss')\n",
    "    ax.plot(history['val_loss'], label='Validation loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4]\n",
    "\n",
    "print(\"Train input shape: \", train_sequenced.shape)\n",
    "train_reconstructed_sequences = [model.predict(train_sequenced) for model in models]\n",
    "for i, sequence in enumerate(train_reconstructed_sequences):\n",
    "    print(f'Recc Train input shape {i + 1}: ', sequence.shape)\n",
    "\n",
    "test_sequenced = create_sequences(test_scaled)\n",
    "\n",
    "print(\"Test input shape: \", test_sequenced.shape)\n",
    "test_reconstructed_sequences = [model.predict(test_sequenced) for model in models]\n",
    "for i, sequence in enumerate(test_reconstructed_sequences):\n",
    "    print(f'Recc Test input shape {i + 1} : ', sequence.shape)\n",
    "\n",
    "train_reconstructed_sequences1, train_reconstructed_sequences2, train_reconstructed_sequences3, train_reconstructed_sequences4 = train_reconstructed_sequences \n",
    "test_reconstructed_sequences1, test_reconstructed_sequences2, test_reconstructed_sequences3, test_reconstructed_sequences4 = test_reconstructed_sequences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_original = []\n",
    "test_descaled = scaler.inverse_transform(test_scaled)\n",
    "test_descaled_original = create_sequences(test_descaled)\n",
    "anomalies = []\n",
    "\n",
    "for sequence in test_descaled_original:\n",
    "    if np.any(sequence > upper_limit):\n",
    "        anomalies_original.append(True)\n",
    "    else:\n",
    "        anomalies_original.append(False)\n",
    "anomalies_original = np.array(anomalies_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "fragment = 30\n",
    "all_original = np.concatenate([train_sequenced[i] for i in range(fragment)])\n",
    "\n",
    "num_plots_per_row = 2\n",
    "num_rows = (len(train_reconstructed_sequences) + num_plots_per_row - 1) // num_plots_per_row\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_plots_per_row, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(len(train_reconstructed_sequences)):\n",
    "    train_reconstructed_sequence = train_reconstructed_sequences[i]\n",
    "    all_reconstructed = np.concatenate([train_reconstructed_sequence[j] for j in range(fragment)])\n",
    "    ax = axes[i]\n",
    "    ax.plot(all_original, label='Original')\n",
    "    ax.plot(all_reconstructed, label='Reconstructed')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Comparison: Original vs Reconstructed {i + 1}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "num_plots_per_row = 2\n",
    "num_rows = (len(train_reconstructed_sequences) + num_plots_per_row - 1) // num_plots_per_row\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_plots_per_row, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "thresholds = []\n",
    "percentile = 99.5 #MODIFICABLE\n",
    "\n",
    "for i in range(len(train_reconstructed_sequences)):\n",
    "    train_reconstructed_sequence = train_reconstructed_sequences[i]\n",
    "    if(LOSS1 == 'MAE'):\n",
    "        train_loss = np.mean(np.abs(train_reconstructed_sequence - train_sequenced), axis=1) #MAE\n",
    "        label = 'MAE'\n",
    "    if(LOSS1 == 'MSE'):\n",
    "        train_loss = np.mean(np.power(train_sequenced - train_reconstructed_sequence, 2), axis=1) #MSE\n",
    "        label = 'MSE'\n",
    "    threshold = np.percentile(train_loss, percentile)\n",
    "    thresholds.append(threshold)\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.plot(train_loss, label=label + ' Loss')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'{label} Loss Model {i + 1}')\n",
    "    ax.legend()\n",
    "    print(f'Reconstruction error threshold {i + 1} (Upper, {percentile}th percentile): ', threshold)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "threshold1, threshold2, threshold3, threshold4 = thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_list = []\n",
    "\n",
    "for i, (test_reconstructed_sequence, threshold) in enumerate(zip(test_reconstructed_sequences, thresholds)):\n",
    "    if(LOSS2 == 'MAE'):\n",
    "        test_loss = np.mean(np.abs(test_reconstructed_sequence - test_sequenced), axis=1) #MAE\n",
    "    if(LOSS2 == 'MSE'):\n",
    "        test_loss = np.mean(np.power(test_sequenced - test_reconstructed_sequence, 2), axis = 1 ) #MSE\n",
    "    test_loss = test_loss.reshape((-1))\n",
    "\n",
    "    anomalies = test_loss > threshold\n",
    "    anomalies_list.append(anomalies)\n",
    "\n",
    "anomalies1, anomalies2, anomalies3, anomalies4 = anomalies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_list = [anomalies_original, anomalies1, anomalies2, anomalies3, anomalies4]\n",
    "\n",
    "fig, axes = plt.subplots(len(anomalies_list), 1, figsize=(12, 5 * len(anomalies_list)))\n",
    "\n",
    "for i, anomalies in enumerate(anomalies_list):\n",
    "    ax = axes[i]\n",
    "    ax.plot(test_descaled, label='Original values')\n",
    "    anomaly_indices = np.where(anomalies)[0]\n",
    "    ax.scatter(anomaly_indices, test_descaled[anomaly_indices], color='red', label='Anomalies')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('co2(PPM)')\n",
    "    ax.set_title(f'Anomalies in Original Data model {i}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_list = [anomalies1, anomalies2, anomalies3, anomalies4]\n",
    "\n",
    "def compute_metrics(original, predicted, model_num):\n",
    "    original = np.array(original).astype(int)\n",
    "    predicted = np.array(predicted).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(original, predicted)\n",
    "    f1 = f1_score(original, predicted)\n",
    "    precision = precision_score(original, predicted)\n",
    "    auc_roc = roc_auc_score(original, predicted)\n",
    "    recall = recall_score(original, predicted)\n",
    "    \n",
    "    print(f'Model {model_num}', TRATAMIENTO, NORMAL, LOSS_FUNCTION, LOSS1, LOSS2, percentile, \"Accuracy:\", accuracy, \"F1 Score:\", f1, \"Precision:\", precision, \"AUC-ROC:\", auc_roc, \"Recall:\", recall)\n",
    "\n",
    "\n",
    "for i, model_anomalies in enumerate(anomalies_list):\n",
    "    compute_metrics(anomalies_original, model_anomalies, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_confusion = []\n",
    "\n",
    "for model_anomalies in anomalies_list:\n",
    "    confusion = confusion_matrix(anomalies_original, model_anomalies)\n",
    "    matrices_confusion.append(confusion)\n",
    "\n",
    "for i, confusion in enumerate(matrices_confusion):\n",
    "    print(f\"Modelo {i+1}:\")\n",
    "    print(confusion)\n",
    "    print(\"\\n\")\n",
    "\n",
    "labels = ['Normal', 'Anomalía']\n",
    "\n",
    "num_rows = (len(matrices_confusion) + 1) // 2\n",
    "num_cols = min(2, len(matrices_confusion))\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6 * num_rows))\n",
    "\n",
    "for i, confusion in enumerate(matrices_confusion):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    df_confusion = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "    ax = axes[row, col]\n",
    "    sns.heatmap(df_confusion, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f\"Matriz de confusión - Modelo {i+1}\")\n",
    "    ax.set_xlabel('Clase Predicha')\n",
    "    ax.set_ylabel('Clase Real')\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.save(f'model_{i+1}.h5')\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    with open(f'model_{i+1}.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos = ['model_1.tflite', 'model_2.tflite', 'model_3.tflite', 'model_4.tflite']\n",
    "for archivo in archivos:\n",
    "    ruta_archivo = os.path.join('', archivo)\n",
    "    tamaño_bytes = os.path.getsize(ruta_archivo)\n",
    "    tamaño_kb = tamaño_bytes / 1024\n",
    "    print(f\"Tamaño de {archivo} en KB: {tamaño_kb} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lite = tf.lite.Interpreter(model_path='model_2.tflite')\n",
    "model_lite.allocate_tensors()\n",
    "input_details = model_lite.get_input_details()\n",
    "output_details = model_lite.get_output_details()\n",
    "\n",
    "input_data1 = test_sequenced[2709:2710].astype(np.float32)\n",
    "model_lite.set_tensor(input_details[0]['index'], input_data1)\n",
    "model_lite.invoke()\n",
    "output_data = model_lite.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n",
    "\n",
    "test_loss = np.mean(np.power(input_data1 - output_data, 2), axis = 1 ) #MSE\n",
    "test_loss = test_loss.reshape((-1))\n",
    "anomaly = test_loss > threshold2\n",
    "print(\"Es una anomalia? -> \" , anomaly)\n",
    "\n",
    "input_data = test_sequenced[2711:2712].astype(np.float32)\n",
    "model_lite.set_tensor(input_details[0]['index'], input_data)\n",
    "model_lite.invoke()\n",
    "output_data = model_lite.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n",
    "\n",
    "test_loss = np.mean(np.power(input_data - output_data, 2), axis = 1 ) #MSE\n",
    "test_loss = test_loss.reshape((-1))\n",
    "anomaly1 = test_loss > threshold2\n",
    "print(\"Es una anomalia? -> \" , anomaly1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
